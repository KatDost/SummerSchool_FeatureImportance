{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance with SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap            # <--- Documentation: https://shap.readthedocs.io/en/latest/\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 -- Data Exploration & Preprocessing (copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adult dataset\n",
    "data_adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "X_adult = data_adult.data\n",
    "y_adult = data_adult.target\n",
    "feature_subset = ['age', 'education-num', 'race', 'sex', 'hours-per-week']\n",
    "X_adult = X_adult[feature_subset]\n",
    "categorical_cols = X_adult.select_dtypes(include=\"category\").columns\n",
    "encoder = OrdinalEncoder()\n",
    "X_adult = X_adult.copy()\n",
    "X_adult.loc[:, categorical_cols] = encoder.fit_transform(X_adult[categorical_cols])\n",
    "\n",
    "# Student performance dataset\n",
    "data_students = fetch_ucirepo(id=320) \n",
    "X_students = data_students.data.features \n",
    "Y_students = data_students.data.targets \n",
    "Y_students_G3 = Y_students['G3'] # only the last, final grade (single-target)\n",
    "X_students = pd.get_dummies(X_students) # one-hot encode categorical features\n",
    "selected_features = ['failures', 'absences', 'goout', 'studytime', 'age'] # for visualization purposes, use only a subset of features\n",
    "X_students_sub = X_students[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 -- Feature Importance with SHAP values\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a unified framework for interpreting model predictions.  \n",
    "It assigns each feature an importance value for a given prediction based on ideas from cooperative game theory.\n",
    "\n",
    "Key Ideas:\n",
    "- Each feature is viewed as a \"player\" in a game.\n",
    "- The \"payout\" is the model's prediction.\n",
    "- SHAP computes how much each feature contributes to the prediction, fairly, by averaging over all possible feature combinations.\n",
    "\n",
    "Why SHAP?\n",
    "- Works for both classification and regression.\n",
    "- Offers **global** (feature-level) and **local** (instance-level) explanations.\n",
    "- The TreeExplainer variant is efficient for tree-based models (like Random Forests).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1 -- SHAP Local Importance (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a linear regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_students_sub, Y_students_G3, random_state=42)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Baseline: Always predicts the average grade\n",
    "dummy_reg = DummyRegressor(strategy=\"mean\")\n",
    "dummy_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "baseline_mse = mean_squared_error(y_test, dummy_reg.predict(X_test))\n",
    "model_mse = mean_squared_error(y_test, lr.predict(X_test))\n",
    "\n",
    "print(f\"Baseline Mean-Squared-Error: {baseline_mse:.3f}\")\n",
    "print(f\"Linear Regression Mean-Squared-Error: {model_mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîç Understanding SHAP for Linear Regression\n",
    "We are using a simple linear regression model to predict the final student grade.\n",
    "- Each feature contributes to the prediction through a weighted sum.\n",
    "- The SHAP value for a feature is simply:\n",
    "SHAP = coefficient √ó (feature value ‚àí mean feature value)\n",
    "- The base value is the model‚Äôs intercept (i.e., prediction at average input).\n",
    "\n",
    "We manually compute:\n",
    "- The model's prediction using the intercept and weighted features.\n",
    "- The SHAP explanation, which should match this exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain SHAP values\n",
    "explainer = shap.Explainer(lr, X_train)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a test sample whose prediction we'd like to explain\n",
    "i = 5\n",
    "x = X_test.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values for the test sample we picked\n",
    "shap_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for the test sample\n",
    "pred = lr.predict(X_test.iloc[[i]])[0]\n",
    "print(f\"Linear regression prediction: {pred:.2f}\")\n",
    "\n",
    "# base_value + sum(SHAP values)\n",
    "shap_sum = shap_values[i].base_values + shap_values[i].values.sum()\n",
    "print(f\"base_value + sum(SHAP values): {shap_sum:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the sum of SHAP values as a waterfall plot\n",
    "shap.plots.waterfall(shap_values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>‚ùì Question:</b> What does the SHAP waterfall plot tell us?\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary>üí¨ Click to show explanation</summary>\n",
    "\n",
    "<div style=\"border: 1px solid #007acc; background-color: #e6f2ff; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üìà Summary:</b>\n",
    "\n",
    "- Model prediction: 13.22 (sum of student‚Äôs final grades)\n",
    "- Model baseline (E[f(x)]): 11.99 (average prediction across all students)\n",
    "- The model moved from 11.99 ‚Üí 13.22 due to the individual feature values\n",
    "\n",
    "<b>Key contributors:</b>\n",
    "- ‚¨ÜÔ∏è <b>Studytime = 3</b> (i.e., high study time): pushed prediction up by +0.73\n",
    "- ‚¨ÜÔ∏è <b>Failures = 0</b>: pushed prediction up by +0.34\n",
    "- ‚¨ÜÔ∏è <b>Goout = 2</b>: moderate socializing also contributed positively (+0.17)\n",
    "- ‚¨áÔ∏è <b>Absences = 6</b>: slightly lowered the prediction (‚Äì0.01)\n",
    "- ‚û°Ô∏è <b>Age = 16</b>: had negligible effect\n",
    "\n",
    "This student‚Äôs relatively strong study habits and clean record (no failures) helped the model predict a higher total grade. Still, the model underestimates the true outcome of 19.\n",
    "</div>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üîç Task:</b> Find a student the model predicts poorly and understand what went wrong.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "...\n",
    "\n",
    "# Tip: you could look at \n",
    "#   i = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to visualize this: SHAP Force Plot\n",
    "\n",
    "A **SHAP force plot** visually explains a single model prediction by showing how each feature pushes the prediction higher or lower relative to a baseline (the model‚Äôs expected value).\n",
    "\n",
    "- **Red arrows** push the prediction **up** (positive SHAP values).\n",
    "- **Blue arrows** push it **down** (negative SHAP values).\n",
    "- The **baseline** is typically the average model output.\n",
    "- The **final prediction** is the result of adding all SHAP values to the baseline.\n",
    "\n",
    "This helps you see *why* the model made a specific decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot version of local SHAP values\n",
    "shap.plots.force(shap_values[5], matplotlib=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üîç Task:</b> Compare force plots for different students.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2 -- Global Importance (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä SHAP Summary Plot (Bar Version)\n",
    "\n",
    "This plot shows the **overall importance of each feature** in the model‚Äôs predictions.\n",
    "\n",
    "- Each bar represents the **mean absolute SHAP value** of a feature across all samples.\n",
    "- Taller bars mean the feature contributed more (on average) to predictions.\n",
    "- This helps identify which features the model relies on most ‚Äî regardless of direction.\n",
    "\n",
    "Use this to **rank feature influence** in a global, model-wide sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summary plot for overall feature importance\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêù SHAP Beehive Plot (Summary Plot)\n",
    "\n",
    "The beehive-style summary plot visualizes both the **importance** and the **effect** of each feature across the dataset:\n",
    "\n",
    "- Each point is a sample; color indicates the **feature value** (e.g., blue = low, red = high).\n",
    "- X-position shows the **SHAP value** (positive = pushes prediction up; negative = pushes it down).\n",
    "- Features are sorted by overall impact (mean |SHAP|).\n",
    "\n",
    "This plot helps you understand:\n",
    "- Which features matter most,\n",
    "- Whether high or low values of a feature increase predictions,\n",
    "- How consistently the model responds to changes in input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP summary plot (beehive version)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>‚ùì Question:</b> What insights can we draw from this SHAP beehive plot?\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary>üí¨ Click to show explanation</summary>\n",
    "\n",
    "<div style=\"border: 1px solid #007acc; background-color: #e6f2ff; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üìä Interpretation:</b>\n",
    "\n",
    "- **failures** has the strongest (and most negative) effect on model predictions. Even a small number of failures can significantly lower the predicted grade sum.\n",
    "- **studytime** tends to increase predictions: students who study more usually get better predicted outcomes.\n",
    "- **goout** shows a negative relationship‚Äîhigh values (i.e., going out more) tend to lower predicted grades.\n",
    "- **absences** and **age** have only minor effects in this model.\n",
    "\n",
    "Note how color helps:\n",
    "- **Red dots** (high feature values) and **blue dots** (low values) let us see *how* the feature influences the prediction‚Äînot just how much.\n",
    "\n",
    "</div>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà SHAP Dependence Plots:\n",
    "\n",
    "Dependence plots help us understand how the SHAP value (i.e., a feature's impact on a prediction) varies with the feature‚Äôs value.\n",
    "\n",
    "Each point represents one student. The x-axis shows the value of the feature; the y-axis shows the feature‚Äôs SHAP value (its contribution to the prediction).  \n",
    "Color indicates the value of another interacting feature that most strongly influences the SHAP value.\n",
    "\n",
    "Use these plots to spot trends, thresholds, and interactions between features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test.columns\n",
    "fig, axes = plt.subplots(1, len(features), figsize=(4 * len(features), 4))\n",
    "\n",
    "for ax, feat in zip(axes, features):\n",
    "    shap.dependence_plot(\n",
    "        ind=feat,\n",
    "        shap_values=shap_values.values,\n",
    "        features=shap_values.data,\n",
    "        feature_names=X_test.columns,\n",
    "        ax=ax,\n",
    "        interaction_index=None,\n",
    "        show=False\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>‚ùì Question:</b> Why are all dots in the SHAP dependence plots aligned in a straight line?\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary>üí¨ Click to show explanation</summary>\n",
    "\n",
    "<div style=\"border: 1px solid #007acc; background-color: #e6f2ff; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üìà Explanation:</b>\n",
    "\n",
    "This happens because we're using a <b>linear regression model</b>. \n",
    "</div>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.3 -- SHAP works for any predictive model\n",
    "\n",
    "However, the behavior of SHAP values depends on the model.\n",
    "- For linear models, SHAP values reduce to the weighted contributions of individual features, resulting in purely additive, linear effects ‚Äî as you've seen with perfectly aligned dots in dependence plots.\n",
    "- For nonlinear models (e.g., trees, neural nets), SHAP captures complex interactions and conditional dependencies, often producing curved or scattered patterns in dependence plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model\n",
    "rf = RandomForestRegressor(random_state=42, n_estimators=500)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer_rf = shap.Explainer(rf, X_train)\n",
    "shap_values_rf = explainer_rf(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependence plots\n",
    "fig, axes = plt.subplots(1, len(features), figsize=(4 * len(features), 4))\n",
    "\n",
    "for ax, feat in zip(axes, features):\n",
    "    shap.dependence_plot(\n",
    "        ind=feat,\n",
    "        shap_values=shap_values_rf.values,\n",
    "        features=X_test,\n",
    "        feature_names=X_test.columns,\n",
    "        ax=ax,\n",
    "        show=False,\n",
    "        interaction_index=None  # no hue\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>‚ùì Question:</b> What patterns can we observe here that would not have been visible in the other types of plots?\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary>üí¨ Click to show explanation</summary>\n",
    "\n",
    "<div style=\"border: 1px solid #007acc; background-color: #e6f2ff; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üìà Summary:</b>\n",
    "\n",
    "Unlike bar or beehive plots, SHAP dependence plots reveal how the *same feature value* can lead to different impacts depending on the context. This is only possible with non-linear models like Random Forests.\n",
    "\n",
    "- For example, **failures = 1** results in SHAP values ranging from ~0 to -5, depending on other features‚Äîhighlighting interaction effects.\n",
    "- **studytime** also shows spread: the same number of hours studied can be helpful or not depending on the rest of the student‚Äôs profile.\n",
    "\n",
    "This variation was *not visible* in the linear model‚Äôs plots, where all points lay on a line. The dependence plot is essential for uncovering such **context-dependent feature contributions**.\n",
    "\n",
    "</div>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.4 -- Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #ffcc00; background-color: #fff8e1; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>üß† Task:</b> Use SHAP values to analyze model predictions on the <code>Adult</code> dataset. \n",
    "</br>\n",
    "\n",
    "\n",
    "Try:\n",
    "<ul>\n",
    "  <li>Inspecting <b>global feature importances</b> using summary plots</li>\n",
    "  <li>Creating <b>dependence plots</b> to visualize feature effects</li>\n",
    "  <li>Generating <b>force plots</b> or <b>waterfall plots</b> to understand wrong predictions on individual samples</li>\n",
    "</ul>\n",
    "\n",
    "Do you think, the model is fair? Or can you detect biases?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_adult, y_adult, random_state=42)\n",
    "\n",
    "# Train a classifier -- how about Gradient Boosting this time??\n",
    "clf = HistGradientBoostingClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Verify that the model works\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "baseline_acc = accuracy_score(y_test, dummy_clf.predict(X_test))\n",
    "model_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(f\"Baseline accuracy: {baseline_acc:.3f}\")\n",
    "print(f\"Gradient Boosting accuracy: {model_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values\n",
    "explainer = shap.Explainer(clf, X_train)\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid #007acc; background-color: #e6f2ff; padding: 10px; border-radius: 5px; color: black;\">\n",
    "<b>‚ÑπÔ∏è Want to explore more?</b><br>\n",
    "Check out <a href=\"https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/\" target=\"_blank\">this excellent non-technical guide</a> by Aidan Cooper.  \n",
    "It walks through SHAP analysis in detail using a different dataset and provides great examples and intuition.\n",
    "\n",
    "Also worth a look: The <a href=\"https://shap.readthedocs.io/en/stable/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html#\" target=\"_blank\">SHAP documentation</a> with lots of detailed explanations and interactive examples.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
